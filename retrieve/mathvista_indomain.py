"""This is for MathVista, testmini as query, test with response
generated by LLaVA-OneVision as corpus."""

import os
import json
import torch
import clip
import numpy as np
from PIL import Image
from tqdm import tqdm
import faiss

def index_img(model, preprocess):
    embeddings = []
    index_to_image_id = {}
    count = 0

    img_dir = "datasets/math_vista/images"
    for i in tqdm(range(1001, 6142)):
        img_file = f"{i}.jpg"
        img_path = os.path.join(img_dir, img_file)

        if not os.path.exists(img_path):
            continue
        
        image_id = i

        if image_id in index_to_image_id.values():
            continue

        with torch.no_grad():
            image = preprocess(Image.open(img_path)).cuda()
            image_embeddings = model.encode_image(torch.unsqueeze(image, dim=0))

        combined_embedding = image_embeddings
        normalized_embedding = combined_embedding / combined_embedding.norm(
            dim=-1, keepdim=True
        )
        embeddings.append(normalized_embedding.cpu().numpy())

        # Map the current index to the image_id
        index_to_image_id[count] = image_id
        count += 1

    embeddings = np.vstack(embeddings).astype('float32')

    # Initialize FAISS index with ID map
    index = faiss.IndexFlatIP(embeddings.shape[1])
    index_with_ids = faiss.IndexIDMap(index)

    # Convert image IDs to int64 and add with IDs to the FAISS index
    ids = np.array(list(index_to_image_id.values())).astype('int64')
    index_with_ids.add_with_ids(embeddings, ids)

    faiss.write_index(index_with_ids, 'indexes/mathvista_image_test.index')
    

def index_txt(model):
    embeddings = []
    index_to_text_id = {}
    count = 0

    data = json.load(open("datasets/math_vista/test.json"))
    for pid, datum in tqdm(data.items()):
        text_id = int(pid)
        if 1001 > text_id:
            continue
        question = datum["question"]

        if text_id in index_to_text_id.values():
            continue

        with torch.no_grad():
            text_tokens = clip.tokenize([question], truncate=True)
            text_features = model.encode_text(text_tokens.cuda())

        text_features /= text_features.norm(dim=-1, keepdim=True)
        text_embeddings = text_features.cpu().detach().numpy().astype('float32')

        embeddings.append(text_embeddings)

        # Map the current index to the text
        index_to_text_id[count] = text_id
        count += 1

    embeddings = np.vstack(embeddings).astype('float32')

    # Initialize FAISS index with ID map
    index = faiss.IndexFlatIP(embeddings.shape[1])
    index_with_ids = faiss.IndexIDMap(index)

    # Convert image IDs to int64 and add with IDs to the FAISS index
    ids = np.array(list(index_to_text_id.values())).astype('int64')
    index_with_ids.add_with_ids(embeddings, ids)

    faiss.write_index(index_with_ids, 'indexes/mathvista_text_test.index')


def image_to_image(image, model, preprocess, ind, topk=5):
    """ Retrieves similar images from the index using the image embeddings.
    Returns the distance and index of the topk similar images.

    Args:
        image (PIL.Image): Image to retrieve similar images.
        model (CLIP): CLIP model.
        preprocess (torchvision.transforms): Image preprocessing pipeline.
        ind (faiss.Index): Faiss index.
        topk (int): Number of similar images to retrieve.
    """
    with torch.no_grad():
        img = preprocess(image).unsqueeze(0).cuda()
        image_features = model.encode_image(img)

        image_features /= image_features.norm(dim=-1, keepdim=True)
        image_embeddings = image_features.cpu().detach().numpy().astype('float32')

        D, I = ind.search(image_embeddings, topk)

    return D, I


def text_to_text(text, model, ind, topk=5):
    """ Retrieves similar text from the index using the text embeddings.
    Returns the distance and index of the topk similar text.
    
    Args:  
        text (str): Text to retrieve similar text.
        model (CLIP): CLIP model.
        ind (faiss.Index): Faiss index.
        topk (int): Number of similar text to retrieve.
    """
    with torch.no_grad():
        text_tokens = clip.tokenize([text], truncate=True)
        text_features = model.encode_text(text_tokens.cuda())

        text_features /= text_features.norm(dim=-1, keepdim=True)
        text_embeddings = text_features.cpu().detach().numpy().astype('float32')

        D, I = ind.search(text_embeddings, topk)

    return D, I


def main():
    clip_model, preprocess = clip.load('ViT-L/14@336px', device='cuda', jit=False)

    # index_img(clip_model, preprocess)
    # index_txt(clip_model)


    data = json.load(open("datasets/math_vista/test.json"))

    i2i_results = {}
    t2t_results = {}

    for pid, datum in tqdm(data.items(), desc="Retrieving from MathVista"):
        if int(pid) > 1000:
            continue
        
        image = Image.open("datasets/math_vista/" + datum["image"])
        img_idx = faiss.read_index("indexes/mathvista_image_test.index")
        D, I = image_to_image(image, clip_model, preprocess, 
                              img_idx, topk=5)
        new_item = datum.copy()
        results = []
        for index, distance in zip(I[0], D[0]):
            result = data.get(str(index))
            results.append(result)

        new_item['retrieved_data'] = results
        i2i_results[pid] = new_item

        question = datum["question"]
        txt_idx = faiss.read_index("indexes/mathvista_text_test.index")
        D, I = text_to_text(question, clip_model, 
                            txt_idx, topk=5)
        
        new_item = datum.copy()
        results = []
        for index, distance in zip(I[0], D[0]):
            result = data.get(str(index))
            results.append(result)

        new_item['retrieved_data'] = results
        t2t_results[pid] = new_item

    with open('output/retrieved/mathvista_test_i2i.json', 'w') as f:
        json.dump(i2i_results, f, indent=4)
    with open('output/retrieved/mathvista_test_t2t.json', 'w') as f:
        json.dump(t2t_results, f, indent=4)

if __name__ == "__main__":
    main()